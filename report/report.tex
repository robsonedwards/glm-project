
\documentclass[11pt, oneside]{article}
\usepackage{geometry} 
\geometry{a4paper} 
%\usepackage[parfill]{parskip}  % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}	
\usepackage{amssymb}
\usepackage{xcolor}

\pagenumbering{gobble}

\definecolor{codegray}{gray}{0.88}
\newcommand \Rcode[1]{{\texttt{\colorbox{codegray}{#1}}}}
%\newcommand \Rcode[1]{{\texttt{{#1}}}}

\title{Finding Pulsars with a Generalised Linear Model}
%\author{Robson Edwards}
\author{Matriculation Number 150017877}
\date{November 27, 2018}

\begin{document}

\maketitle

%If you fit a model to some data, consider whether there is a need for goodness-of-fit tests. State the assumptions of the methods you use, and consider how likely they are to be met – see the above advice on the Discussion section for more on this.

\section{Executive Summary}

We ...

\section{Introduction}

What is the motivation for the work?

What is the data? 

What are the purpose and/or objectives of the work/analysis?

What methods are commonly used for this sort of analysis? How does the analysis align or differ from these methods? 

Note that we are most sensitive about false negatives than false positives. 

\section{Methods}

Add a brief summary of Methods

\subsection{Data}
\label{subsec:data}

The data are the HTRU2 dataset, ``which describes a sample of pulsar candidates collected during the High Time Resolution Universe Survey.'' \cite{data} It was downloaded from the UC Irvine Machine Learning repository.  Pulsars are a rare and interesting type of neutron star which rotate at a fast rate and transmit a periodic signal which can be detected by large radio telescopes on Earth. However, similar signals can be generated by noise or radio-frequency interference. Hence, classification of genuine pulsars from so-called ``pulsar candidates'' is an area of considerable scientific interest.

The data consist of 17,898 ``pulsar candidates'' (rows). For each candidate we have eight continuous variables (henceforth V1 through V8) and one binary class variable, for a total of nine columns. V1 through V4 describe features of the integrated profile of the emission pattern from different pulsar candidates. V5 through V8 describe features of the candidate's DM-SNR curves (dispersion measure and signal-to-noise ratio). \cite{data} While the specific details of these eight variables are interesting, they are outside the scope of this course and paper. The last variable is binary and labels of the candidate's class (authentic pulsar or spurious candidate). Fortuitously, every pulsar candidate is labelled. We treat this as a supervised learning problem, where the eight explanatory variables are used to predict class. 

We plot relationships between the eight explanatory variables in figure {\color{red} TODO: insert figure}. It can be seen, roughly speaking, that the pulsars (cyan) and non-pulsars (orange) are well-separated on many axes. Hence, we imagine that we could address our prediction problem with a binomial GLM. 

There are at least two problems with the data. First, the explanatory variables are not really interpretable, at least with the author's level of understanding of signal processing and of astronomy. This is not a massive obstacle, as for our purposes we are more interested in prediction than inference, so our final model does not need to be highly interpretable. The second problem is multicollinearity. It can be seen from figure {\color{red} TODO: insert figure} that there is a high correlation between V3 and V4, and between V7 and V8. In fact, the correlations (Pearson product moment coefficient) are 0.95 and 0.92, respectively. We will address this issue later. All other absolute correlations are below 0.9. 

\subsection{Model Formulation}

We consider how to fit a generalised linear model to the data. Our first intuition is to fit a model with all eight explanatory variables and no interactions or higher-order terms. This model assumes that pulsar candidates are distributed according to a Bernoulli distribution where $p$, the probability of being a genuine pulsar, is logit-linear to a weighted sum of V1-V8. 

This initial model, henceforth Model 1, works reasonably well. Model 1 has AIC 2633.8. If we consider the natural classifier suggested by this model, where we classify all candidates with predicted $\hat{p} > 0.5$ as pulsars and with $\hat{p} \leq 0.5$ as non-pulsars, we can quantify the quality of that classifier. We henceforth refer to the \emph{accuracy} of a classifier as the overall proportion of correctly classified candidates, the \emph{specificity} as the proportion of non-pulsars which it correctly rejects, and the \emph{sensitivity} as the proportion of pulsars which it correctly accepts. Then, Model 1 gives rise to ``Classifier 1'', which has accuracy 0.980, sensitivity 0.828, and specificity 0.995.  This is pretty good for a first start. 

We plot the accuracy, sensitivity and specificity of the space of all classifiers derived from Model 1 with different cutoffs between 0 and 1 in {\color{red} TODO: insert figure}.

However, there are three main problems with our first model. First, Classifier 1 is too conservative in classifying pulsar candidates. We note that we can improve this by reducing the classification cutoff (which was $\hat{p} > 0.5$ above). If we select the cutoff to maximize accuracy, we find that a cutoff $\hat{p} > 0.364$ gives us an improved classifier ``Classifier 1B'' with accuracy 0.981, sensitivity 0.853, and specificity 0.993. We consider this to be a better classifier because of the improved accuracy and significantly improved sensitivity. We are particularly interested in sensitivity, because our goal is to find pulsars, and so the cost of missing a true pulsar is ``worse'' than incorrectly accepting a non-pulsar. Thus, roughly speaking, gaining 0.025 sensitivity is ``worth'' losing 0.002 specificity. 

Note that it would not be reasonable to select the cutoff by maximizing sensitivity, as this would simply give us a cutoff of 0 with sensitivity 1, and accept every non-pulsar. It would also be absurd to maximize specificity, as this would give us a cutoff of 1 and reject every pulsar. Hence, we maximize accuracy. There exist other reasonable metrics of classifier quality but accuracy is sufficient for our purposes. 

The second problem is that the effects of V7 and V8 are not significant in the model. The p-value for the effect of V7 is 0.58 and for V8, 0.12. We will address this momentarily. 

\begin{table}[h!]
\centering
\begin{tabular}{|c c c c c c c c|} 
 \hline
  V1  &  V2 &  V3 &   V4 &   V5&    V6  &  V7  &  V8 \\
  \hline
 4.21 & 1.73 &12.35 & 6.98 & 3.96 & 9.61& 37.03 &15.88 \\
  \hline
\end{tabular}
\caption{Variance Inflation Factors}
\label{table:vif}
\end{table}


The third problem is the collinearity issue identified in \S\ref{subsec:data}. V3 and V4 are highly correlated and V7 and V8 are highly correlated. We calculate the variance inflation factors (VIFs) for Model 1 and present them in table \ref{table:vif}. We recall from lectures that 5 or 10 are reasonable cutoffs for VIF. V3, V7, and V8 have the largest VIFs, so we now fit and analyse three more models, one each without V3, V7, and V8. Each of these models has all significant effects. The model without V8 has AIC 2634.2. The model without V7 has AIC 2632.1. 

The model without V3, unfortunately, has some numerical issues. Eleven of the pulsars are fitted to linear predictor values greater than 30. These give fitted values which are numerically equivalent. (Fair enough, $e^{30} / (1 + e^{30})$ is extremely close to 1.) This model has AIC 3136.3. Also, the natural classifier it suggests has accuracy 0.975, sensitivity 0.774, and specificity 0.995. This is much worse than what we've been working with so far. 
I am not sure how to remedy the numerical issues. I know that we \emph{do not} have perfect separation, which can give rise to a similar error. I know this because the greatest fitted value among non-pulsars is $\hat{p} = 0.9999993$, and the least among pulsars is $\hat{p} = 0.00104$. So there is some overlap in the classes on the predictor scale under this model. 

We decide that the model without V7, henceforth ``Model 2'', is the best yet. Conducting a stepwise selection by AIC, starting with Model 1, also selects Model 2. 

We can consider larger models. 

First, we consider ``Model 3'', which is the model with all eight explanatory variables and also all 28 interactions between them. 

\subsection{Model Selection}

LRTs aim at identifying the model that best explains the data. AIC aims at identifying the model that leads to optimal predictions. From that point of view, it makes most sense to use LRTs when the primary aim is to understand the relationship between response and covariates, while the AIC should be preferred if accurate predictions are desired (5.32)

Hence, we use AIC. 

\section{Results}

What are the main findings of the analysis? Try to relate these findings to the objectives you outlined in the introduction. Include both significant and non-significant findings in your report. Be sure to use appropriate language so that the reader does not mistake significant findings for substantial (practically significant) findings.

Quantify any findings of your analysis – e.g. using confidence intervals described using non-technical language.

How well do the model(s) describe the data? If model performance is poor, how/in what situations is it poor?

Compare to other methods for finding pulsars and cite those other papers 

\section{Conclusion}

What, if anything, can you conclude based on the results? 

What are the restrictions for any conclusions drawn based on this analysis?

Are you happy that the results summarised are defensible? If there are any issues with model validity, how might these be affecting the results in this case? Be sure to use non-technical language.

Has this work highlighted issues that could be addressed with further work? If so, what would recommend the next steps/future work entails?

\begin{thebibliography}{9}

\bibitem{R}{
R Core Team (2018). 
\textit{R: A language and environment for statistical
computing.} R Foundation for Statistical Computing, Vienna, Austria.
URL https://www.R-project.org/.
}
 
{\color{red}Also cite my data }
 
\end{thebibliography}

\section*{Appendix: R Code}

\end{document}
